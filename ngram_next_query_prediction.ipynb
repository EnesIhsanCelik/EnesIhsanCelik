{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFV6YT0PYT85"
      },
      "source": [
        "## Preprocessing Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N8nSQ6_YT86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "def preprocess_aol_query_log(input_dir):\n",
        "\n",
        "    start_time = time.time()\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "    # Regular expression to find queries consisting of only special characters\n",
        "    special_chars_only_pattern = re.compile(r'^[_\\W\\s]*$')\n",
        "\n",
        "    total_processed_lines = 0\n",
        "    total_duplicates_removed = 0\n",
        "    total_special_chars_removed = 0\n",
        "    total_malformed_ids_removed = 0\n",
        "    total_malformed_lines_skipped = 0\n",
        "    total_empty_lines_skipped = 0\n",
        "    total_files = 0\n",
        "\n",
        "    # Sets to track userIDs\n",
        "    total_userids = set()\n",
        "    remaining_userids = set()\n",
        "\n",
        "    # Create output directory and sub-directory for processed files\n",
        "    output_dir = f\"aol_processed\"\n",
        "    processed_dir = os.path.join(output_dir, \"processed_files\")\n",
        "    if not os.path.exists(processed_dir):\n",
        "        os.makedirs(processed_dir)\n",
        "\n",
        "    # Create filepaths for output files\n",
        "    special_char_output_file = os.path.join(\n",
        "        output_dir, \"special_char_queries.txt\")\n",
        "    malformed_id_output_file = os.path.join(\n",
        "        output_dir, \"malformed_id_queries.txt\")\n",
        "    stats_output_file = os.path.join(output_dir, \"processing_stats.txt\")\n",
        "\n",
        "    # Open miscellaneous output files for writing\n",
        "    with open(special_char_output_file, 'w', encoding='utf-8') as special_char_file, \\\n",
        "            open(malformed_id_output_file, 'w', encoding='utf-8') as malformed_id_file, \\\n",
        "            open(stats_output_file, 'w', encoding='utf-8') as stats_file:\n",
        "\n",
        "        print(f\"AOL Query Log Processing - Started at {timestamp}\\n\")\n",
        "        stats_file.write(\n",
        "            f\"AOL Query Log Processing - Started at {timestamp}\\n\")\n",
        "\n",
        "        for filename in os.listdir(input_dir):\n",
        "            if not filename.endswith('.txt'):\n",
        "                continue\n",
        "\n",
        "            total_files += 1\n",
        "\n",
        "            # Create pathnames for input file and output file\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "            output_path = os.path.join(processed_dir, filename)\n",
        "\n",
        "            print(f\"Processing file {filename}\")\n",
        "            stats_file.write(f\"Processing file {filename}\\n\")\n",
        "\n",
        "            file_processed_lines = 0\n",
        "            file_duplicates_removed = 0\n",
        "            file_special_chars_removed = 0\n",
        "            file_malformed_ids_removed = 0\n",
        "            file_malformed_lines_skipped = 0\n",
        "            file_empty_lines_skipped = 0\n",
        "\n",
        "            # Open input file and create output file\n",
        "            try:\n",
        "                with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "\n",
        "                    next(infile)  # Skip the header\n",
        "\n",
        "                    prev_anon_id = None\n",
        "                    prev_query = None\n",
        "\n",
        "                    for line in infile:\n",
        "                        file_processed_lines += 1\n",
        "\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            file_empty_lines_skipped += 1\n",
        "                            continue  # Skip empty lines\n",
        "\n",
        "                        parts = line.split('\\t')\n",
        "                        if len(parts) < 3:\n",
        "                            file_malformed_lines_skipped += 1\n",
        "                            continue  # Skip malformed lines\n",
        "\n",
        "                        anon_id = parts[0].strip()\n",
        "                        query = parts[1].strip()\n",
        "\n",
        "                        if not anon_id.isdigit():\n",
        "                            file_malformed_ids_removed += 1\n",
        "                            malformed_id_file.write(\n",
        "                                f\"{line}\\t{filename}\\n\")\n",
        "                            # Skip malformed anonIDs\n",
        "                            continue\n",
        "\n",
        "                        total_userids.add(anon_id)\n",
        "\n",
        "                        is_duplicate = (\n",
        "                            anon_id == prev_anon_id and query == prev_query)\n",
        "\n",
        "                        is_special_chars_only = bool(\n",
        "                            special_chars_only_pattern.match(query))\n",
        "\n",
        "                        if is_duplicate:\n",
        "                            file_duplicates_removed += 1\n",
        "                        elif is_special_chars_only:\n",
        "                            file_special_chars_removed += 1\n",
        "                            special_char_file.write(line + '\\n')\n",
        "                        else:\n",
        "                            # Only keep a query if its unique and not only consisting of special characters.\n",
        "                            # Modification: Since some rows have 3 columns of data and others 5,\n",
        "                            # we remove the columns for ClickURL and ItemRank so that pandas can create a dataframe\n",
        "                            # from the input data, and since we don't use them anyway.\n",
        "                            outfile.write(anon_id + \"\\t\" + query + '\\n')\n",
        "                            remaining_userids.add(anon_id)\n",
        "\n",
        "                        prev_anon_id = anon_id\n",
        "                        prev_query = query\n",
        "\n",
        "                    total_processed_lines += file_processed_lines\n",
        "                    total_duplicates_removed += file_duplicates_removed\n",
        "                    total_special_chars_removed += file_special_chars_removed\n",
        "                    total_malformed_ids_removed += file_malformed_ids_removed\n",
        "                    total_malformed_lines_skipped += file_malformed_lines_skipped\n",
        "                    total_empty_lines_skipped += file_empty_lines_skipped\n",
        "\n",
        "                    file_stats = [\n",
        "                        f\"  - Processed: {file_processed_lines:,} queries\",\n",
        "                        f\"  - Skipped {file_empty_lines_skipped:,} empty lines\",\n",
        "                        f\"  - Skipped {file_malformed_lines_skipped:,} malformed lines\",\n",
        "                        f\"  - Removed {file_malformed_ids_removed:,} queries with malformed IDs\",\n",
        "                        f\"  - Removed {file_duplicates_removed:,} duplicate queries\",\n",
        "                        f\"  - Removed {file_special_chars_removed:,} special-character-only queries\",\n",
        "                        f\"  - Remaining: {file_processed_lines - file_duplicates_removed - file_special_chars_removed - file_malformed_ids_removed:,} queries\\n\"\n",
        "                    ]\n",
        "\n",
        "                    # Print and write the file stats\n",
        "                    for stat in file_stats:\n",
        "                        print(stat)\n",
        "                        stats_file.write(stat + \"\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"Error processing {filename}: {str(e)}\"\n",
        "                print(error_msg)\n",
        "                stats_file.write(error_msg + \"\\n\")\n",
        "\n",
        "        remaining = total_processed_lines - total_duplicates_removed - \\\n",
        "            total_special_chars_removed - total_malformed_ids_removed\n",
        "\n",
        "        summary_stats = [\n",
        "            \"\\n\" + \"=\"*50,\n",
        "            \"PROCESSING COMPLETE\",\n",
        "            \"=\"*50,\n",
        "            f\"Processed {total_files} files with {total_processed_lines:,} total queries\",\n",
        "            f\"Skipped {total_empty_lines_skipped:,} empty lines\",\n",
        "            f\"Skipped {total_malformed_lines_skipped:,} malformed lines\",\n",
        "            f\"Removed {total_malformed_ids_removed:,} queries with malformed IDs\",\n",
        "            f\"Removed {total_duplicates_removed:,} duplicate queries ({total_duplicates_removed/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Removed {total_special_chars_removed:,} special-char queries ({total_special_chars_removed/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Remaining non-duplicate, valid ID queries: {total_processed_lines - total_duplicates_removed - total_malformed_ids_removed:,} ({(total_processed_lines - total_duplicates_removed - total_malformed_ids_removed)/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Remaining non-duplicate, valid ID, non-special-char queries: {remaining:,} ({remaining/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Removed userIDs: {len(total_userids) - len(remaining_userids)}\",\n",
        "            f\"Remaining UserIDs after processing: {len(remaining_userids)}\",\n",
        "            \"=\"*50,\n",
        "        ]\n",
        "\n",
        "        for stat in summary_stats:\n",
        "            print(stat)\n",
        "            stats_file.write(stat + \"\\n\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"\\nProcessing completed at {datetime.datetime.now()}\\n\")\n",
        "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "        stats_file.write(\n",
        "            f\"\\nProcessing completed at {datetime.datetime.now()}\\n\")\n",
        "        stats_file.write(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Preprocess AOL query log files\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"input_dir\", help=\"Path to input directory containing AOL query log files\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    preprocess_aol_query_log(args.input_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaAvzawvYT87"
      },
      "source": [
        "## Vocabulary Creation ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXnYr-duYT88"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import kenlm\n",
        "\n",
        "# === Paths ===\n",
        "BASE_PATH = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/aol_processed/\"\n",
        "INPUT_DIR = BASE_PATH + \"processed_files/\"\n",
        "TOKENIZED_FILE_PATH = BASE_PATH + \"queries_tokenized.txt\"\n",
        "VOCAB_DICT_PATH = BASE_PATH + \"vocab_dict.json\"\n",
        "VOCAB_STATS_PATH = BASE_PATH + \"vocab_stats.json\"\n",
        "\n",
        "\n",
        "def tokenize_query(query):\n",
        "\n",
        "    # Treat '.' as a separate token\n",
        "    query_with_spaced_dots = query.replace('.', ' . ')\n",
        "\n",
        "    # Split on whitespace\n",
        "    tokens = query_with_spaced_dots.split()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def tokenize_and_create_vocab(input_dir, vocab_size=45000):\n",
        "    word_counts = Counter()\n",
        "\n",
        "    with open(TOKENIZED_FILE_PATH, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        print(\"Starting tokenization and vocabulary creation...\")\n",
        "\n",
        "        for filename in os.listdir(input_dir):\n",
        "            if not filename.endswith(\".txt\"):\n",
        "                continue\n",
        "\n",
        "            file_path = os.path.join(input_dir, filename)\n",
        "            print(f\"Processing: {filename}\")\n",
        "\n",
        "            for chunk in pd.read_csv(file_path, sep='\\t', names=['userID', 'query'], chunksize=100000):\n",
        "                for query in chunk['query']:\n",
        "                    tokens = tokenize_query(query)\n",
        "                    if tokens:\n",
        "                        word_counts.update(tokens)\n",
        "                        outfile.write(\" \".join(tokens) + \"\\n\")\n",
        "\n",
        "    total_tokens_counted = sum(word_counts.values())\n",
        "    special_tokens = ['<OOV>']\n",
        "    most_common_words = [word for word, _ in word_counts.most_common(vocab_size - len(special_tokens))]\n",
        "\n",
        "    vocabulary = special_tokens + most_common_words\n",
        "    vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "    actual_vocab_size = len(vocab_dict)\n",
        "    covered_tokens_count = sum(word_counts[word] for word in most_common_words)\n",
        "    coverage_percentage = (covered_tokens_count / total_tokens_counted) * 100 if total_tokens_counted > 0 else 0\n",
        "\n",
        "    vocab_stats = {\n",
        "        \"Requested_Vocabulary_Size\": vocab_size,\n",
        "        \"Actual_Vocabulary_Size\": actual_vocab_size,\n",
        "        \"Total_Tokens_Found\": total_tokens_counted,\n",
        "        \"Total_Unique_Tokens_Found\": len(word_counts),\n",
        "        \"Coverage_Percentage_Of_Top_Tokens\": round(coverage_percentage, 2),\n",
        "        \"Special_Tokens\": special_tokens\n",
        "    }\n",
        "\n",
        "    with open(VOCAB_DICT_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(vocab_dict, f)\n",
        "\n",
        "    with open(VOCAB_STATS_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(vocab_stats, f)\n",
        "\n",
        "    print(\"✅ Tokenization complete. Saved to:\", TOKENIZED_FILE_PATH)\n",
        "    print(\"✅ Vocabulary saved to:\", VOCAB_DICT_PATH)\n",
        "    print(\"Vocabulary Stats:\")\n",
        "    print(json.dumps(vocab_stats, indent=4))\n",
        "\n",
        "\n",
        "\n",
        "def get_vocabulary(query_file, vocab_size=45000):\n",
        "    word_counter = Counter()\n",
        "\n",
        "    with open(query_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            words = line.strip().split()\n",
        "            word_counter.update(words)\n",
        "\n",
        "    vocab_dict = dict(word_counter.most_common(vocab_size))\n",
        "\n",
        "    vocab_stats = {\n",
        "        'total_words': sum(word_counter.values()),\n",
        "        'unique_words': len(word_counter),\n",
        "        'vocab_size': len(vocab_dict),\n",
        "    }\n",
        "\n",
        "    return vocab_dict, vocab_stats\n",
        "\n",
        "vocab_size = 45000\n",
        "tokenize_and_create_vocab(INPUT_DIR, vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ElUmifYT88"
      },
      "source": [
        "## Evaluation of n-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHxMUkdNYT88"
      },
      "outputs": [],
      "source": [
        "import kenlm\n",
        "from create_vocab_query import get_query_vocabulary as get_vocabulary\n",
        "\n",
        "\n",
        "def query_level_next_prediction(model_path, eval_file, top_k=5, num_examples=10):\n",
        "    model = kenlm.Model(model_path)\n",
        "\n",
        "    # Load query stream\n",
        "    with open(eval_file, 'r', encoding='utf-8') as f:\n",
        "        queries = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"🔍 Running next-query prediction on {num_examples} examples...\\n\")\n",
        "\n",
        "    vocab_dict = get_vocabulary(eval_file, vocab_size=None)\n",
        "    vocabulary = list(vocab_dict.keys())\n",
        "\n",
        "    mrr_scores = []\n",
        "\n",
        "    for i in range(len(queries) - 2):\n",
        "        context = f\"{queries[i]} {queries[i+1]}\"  # Trigram context: 2 previous queries\n",
        "        true_query = queries[i+2]\n",
        "\n",
        "        # Score all candidate queries\n",
        "        scores = {\n",
        "            q: model.score(f\"{context} {q}\", bos=False, eos=False)\n",
        "            for q in vocabulary\n",
        "        }\n",
        "\n",
        "        sorted_preds = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_preds = [q for q, _ in sorted_preds[:top_k]]\n",
        "\n",
        "        # Calculate MRR for this prediction\n",
        "        if true_query in top_preds:\n",
        "            rank = top_preds.index(true_query) + 1\n",
        "            mrr = 1.0 / rank\n",
        "        else:\n",
        "            rank = None\n",
        "            mrr = 0.0\n",
        "        mrr_scores.append(mrr)\n",
        "\n",
        "        print(f\"🔹 Example {i+1}\")\n",
        "        print(f\"Context     : [{queries[i]}] → [{queries[i+1]}]\")\n",
        "        print(f\"True query  : {true_query}\")\n",
        "        print(f\"Top-{top_k} : {top_preds}\")\n",
        "        print(f\"MRR         : {mrr:.4f}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if i + 1 == num_examples:\n",
        "            break\n",
        "\n",
        "    avg_mrr = sum(mrr_scores) / len(mrr_scores)\n",
        "    print(f\"\\n📊 Average MRR over {len(mrr_scores)} examples: {avg_mrr:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/3gram_query.binary\"\n",
        "    eval_file = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/data/ngram_eval.txt\"\n",
        "    query_level_next_prediction(model_path, eval_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_6bTCJxYT89"
      },
      "source": [
        "## Resource Logger and Measuring Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rWE-VK_YT89"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil\n",
        "import kenlm\n",
        "from pynvml import *\n",
        "from ngram_mrr_evaluation import get_rank_of_true_word\n",
        "from ngram_mrr_evaluation import load_validation_data\n",
        "from create_vocabulary import get_vocabulary\n",
        "\n",
        "# Track resource usage at each step\n",
        "def track_resources():\n",
        "    cpu_usage = psutil.cpu_percent(interval=1)\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    disk_info = psutil.disk_usage('/')\n",
        "\n",
        "    try:\n",
        "        nvmlInit()\n",
        "        handle = nvmlDeviceGetHandleByIndex(0)  # GPU 0\n",
        "        gpu_util = nvmlDeviceGetUtilizationRates(handle)\n",
        "        gpu_memory = nvmlDeviceGetMemoryInfo(handle)\n",
        "        gpu_info = {\n",
        "            'gpu_utilization': gpu_util.gpu,\n",
        "            'gpu_memory_used': gpu_memory.used,\n",
        "            'gpu_memory_percent': gpu_memory.used / gpu_memory.total * 100\n",
        "        }\n",
        "        nvmlShutdown()\n",
        "    except:\n",
        "        gpu_info = None\n",
        "\n",
        "    return {\n",
        "        'cpu': cpu_usage,\n",
        "        'memory_used': memory_info.used,\n",
        "        'memory_percent': memory_info.percent,\n",
        "        'disk_used': disk_info.used,\n",
        "        'disk_percent': disk_info.percent,\n",
        "        'gpu': gpu_info\n",
        "    }\n",
        "\n",
        "# Function to plot the resource usage as a table\n",
        "def plot_resources_as_table(cpu_data, memory_data, disk_data, gpu_data):\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.axis('off')\n",
        "\n",
        "    time_axis = np.arange(len(cpu_data))  # Time axis for rows (number of recorded steps)\n",
        "\n",
        "\n",
        "    table_data = [\n",
        "        ['CPU'] + [f'{x:.2f}' for x in cpu_data],\n",
        "        ['Memory'] + [f'{x:.2f}' for x in memory_data['memory_percent']],\n",
        "        ['Disk'] + [f'{x:.2f}' for x in disk_data['disk_percent']],\n",
        "    ]\n",
        "\n",
        "    if gpu_data:\n",
        "        table_data.append(['GPU'] + [f'{x:.2f}' for x in gpu_data])\n",
        "\n",
        "    table = ax.table(cellText=table_data, colLabels=[f'Time {i+1}' for i in range(len(time_axis))], loc='center', cellLoc='center')\n",
        "\n",
        "    for i, row in enumerate(table.get_celld().values()):\n",
        "        if i == 0:\n",
        "            row.set_facecolor('lightblue')\n",
        "        elif i == 1:\n",
        "            row.set_facecolor('lightgreen')\n",
        "        elif i == 2:\n",
        "            row.set_facecolor('lightcoral')\n",
        "        elif i == 3 and gpu_data:\n",
        "            row.set_facecolor('lightsalmon')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_kenlm_model(models, query_file, n=5, sample_size=1000):\n",
        "    validation_data = load_validation_data(query_file, n=n, sample_size=sample_size)\n",
        "\n",
        "    vocab_dict, vocab_stats = get_vocabulary(query_file=query_file, vocab_size=45000)\n",
        "    vocabulary = list(vocab_dict.keys())\n",
        "\n",
        "    print(f\"Evaluating {len(validation_data)} examples...\")\n",
        "\n",
        "    mrr_scores = {n_gram: [] for n_gram in models.keys()}\n",
        "    accuracy_scores = {n_gram: [] for n_gram in models.keys()}\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    cpu_data = []\n",
        "    memory_data = {'memory_percent': []}\n",
        "    disk_data = {'disk_percent': []}\n",
        "    gpu_data = []\n",
        "\n",
        "    # Evaluate for each N-gram model (2-gram, 3-gram, 4-gram, 5-gram)\n",
        "    for n_gram, model in models.items():\n",
        "        print(f\"Evaluating {n_gram}-gram model...\")\n",
        "\n",
        "        for i, (input_text, true_word) in enumerate(validation_data):\n",
        "            # Track resources during evaluation\n",
        "            resources = track_resources()\n",
        "            cpu_data.append(resources['cpu'])\n",
        "            memory_data['memory_percent'].append(resources['memory_percent'])\n",
        "            disk_data['disk_percent'].append(resources['disk_percent'])\n",
        "            if resources['gpu'] is not None:\n",
        "                gpu_data.append(resources['gpu'])\n",
        "\n",
        "            rank, top_k = get_rank_of_true_word(model, input_text, true_word, vocabulary)\n",
        "\n",
        "            if rank is None:\n",
        "                mrr_scores[n_gram].append(0.0)\n",
        "                accuracy_scores[n_gram].append(0)\n",
        "            else:\n",
        "                mrr_scores[n_gram].append(1.0 / rank)\n",
        "                accuracy_scores[n_gram].append(1 if true_word in top_k else 0)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"... Evaluated {i+1}/{len(validation_data)} examples in {elapsed:.2f}s\")\n",
        "\n",
        "    # Calculate average MRR and accuracy for each model\n",
        "    mrr_averages = {n_gram: np.mean(mrr_scores[n_gram]) for n_gram in mrr_scores}\n",
        "    accuracy_averages = {n_gram: np.mean(accuracy_scores[n_gram]) for n_gram in accuracy_scores}\n",
        "\n",
        "    print(\"Mean Reciprocal Rank (MRR):\")\n",
        "    for n_gram in mrr_averages:\n",
        "        print(f\"{n_gram}-gram: {mrr_averages[n_gram]:.4f}\")\n",
        "\n",
        "    print(\"\\nAccuracy:\")\n",
        "    for n_gram in accuracy_averages:\n",
        "        print(f\"{n_gram}-gram: {accuracy_averages[n_gram]:.4f}\")\n",
        "\n",
        "    plot_metrics(mrr_averages, accuracy_averages)\n",
        "    plot_resources_as_table(cpu_data, memory_data, disk_data, gpu_data)\n",
        "\n",
        "# Function to plot MRR and accuracy\n",
        "def plot_metrics(mrr_averages, accuracy_averages):\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    n_grams = list(mrr_averages.keys())\n",
        "    mrr_values = list(mrr_averages.values())\n",
        "    accuracy_values = list(accuracy_averages.values())\n",
        "\n",
        "    bar_width = 0.35\n",
        "    index = np.arange(len(n_grams))\n",
        "\n",
        "    ax.bar(index, mrr_values, bar_width, label='MRR', color='lightblue')\n",
        "    ax.bar(index + bar_width, accuracy_values, bar_width, label='Accuracy', color='lightgreen')\n",
        "\n",
        "    ax.set_xlabel('N-gram Model')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Evaluation of N-gram Models (MRR and Accuracy)')\n",
        "    ax.set_xticks(index + bar_width / 2)\n",
        "    ax.set_xticklabels([f'{n}-gram' for n in n_grams])\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "models = {\n",
        "    2: kenlm.Model(\"queries_tokenized_2gram.arpa\"),\n",
        "    3: kenlm.Model(\"queries_tokenized_3gram.arpa\"),\n",
        "    4: kenlm.Model(\"queries_tokenized_4gram.arpa\"),\n",
        "    5: kenlm.Model(\"queries_tokenized_5gram.arpa\"),\n",
        "}\n",
        "\n",
        "query_file = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/aol_processed/queries_tokenized.txt\"\n",
        "evaluate_kenlm_model(models, query_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhyVuZ_8Y1AQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}