{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIG1I4qzdoki",
        "outputId": "04fe1e1b-fb69-4f56-f731-57d5c2500f93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Using cached https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y build-essential cmake libboost-all-dev zlib1g-dev libbz2-dev liblzma-dev\n",
        "!git clone https://github.com/kpu/kenlm.git\n",
        "%cd kenlm\n",
        "!mkdir build && cd build && cmake .. && make -j4"
      ],
      "metadata": {
        "id": "U7Qx6TJydyFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huSMBYZjeg7K",
        "outputId": "78930d34-f459-4c05-b8b1-55b9e30c76a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kaVqKSo9pGOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title === Imports & Configuration ===\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "from collections import Counter\n",
        "import kenlm\n",
        "import psutil\n",
        "from google.colab import drive\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === Colab Google Drive Paths ===\n",
        "BASE_PATH = \"/content/drive/MyDrive/dvk-uppsats/\"\n",
        "INPUT_DIR = BASE_PATH + \"aol_processed/processed_files/\"\n",
        "TRAIN_FILE_PATH = BASE_PATH + \"data/ngram_train.txt\"\n",
        "EVAL_FILE_PATH = BASE_PATH + \"data/ngram_eval.txt\"\n",
        "MODEL_PATH = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/3gram_query.binary\" # Detta rad skulle ändras när vi evaluerar olika modeller\n",
        "VOCAB_DICT_PATH = BASE_PATH + \"data/query_vocab_dict.json\"\n",
        "VOCAB_STATS_PATH = BASE_PATH + \"data/query_vocab_stats.json\"\n"
      ],
      "metadata": {
        "id": "4l4l2HgEo2GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFV6YT0PYT85"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_N8nSQ6_YT86"
      },
      "outputs": [],
      "source": [
        "# @title === Preprocessing Script ===\n",
        "def preprocess_aol_query_log(input_dir):\n",
        "\n",
        "    start_time = time.time()\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "    # Regular expression to find queries consisting of only special characters\n",
        "    special_chars_only_pattern = re.compile(r'^[_\\W\\s]*$')\n",
        "\n",
        "    total_processed_lines = 0\n",
        "    total_duplicates_removed = 0\n",
        "    total_special_chars_removed = 0\n",
        "    total_malformed_ids_removed = 0\n",
        "    total_malformed_lines_skipped = 0\n",
        "    total_empty_lines_skipped = 0\n",
        "    total_files = 0\n",
        "\n",
        "    # Sets to track userIDs\n",
        "    total_userids = set()\n",
        "    remaining_userids = set()\n",
        "\n",
        "    # Create output directory and sub-directory for processed files\n",
        "    output_dir = f\"aol_processed\"\n",
        "    processed_dir = os.path.join(output_dir, \"processed_files\")\n",
        "    if not os.path.exists(processed_dir):\n",
        "        os.makedirs(processed_dir)\n",
        "\n",
        "    # Create filepaths for output files\n",
        "    special_char_output_file = os.path.join(\n",
        "        output_dir, \"special_char_queries.txt\")\n",
        "    malformed_id_output_file = os.path.join(\n",
        "        output_dir, \"malformed_id_queries.txt\")\n",
        "    stats_output_file = os.path.join(output_dir, \"processing_stats.txt\")\n",
        "\n",
        "    # Open miscellaneous output files for writing\n",
        "    with open(special_char_output_file, 'w', encoding='utf-8') as special_char_file, \\\n",
        "            open(malformed_id_output_file, 'w', encoding='utf-8') as malformed_id_file, \\\n",
        "            open(stats_output_file, 'w', encoding='utf-8') as stats_file:\n",
        "\n",
        "        print(f\"AOL Query Log Processing - Started at {timestamp}\\n\")\n",
        "        stats_file.write(\n",
        "            f\"AOL Query Log Processing - Started at {timestamp}\\n\")\n",
        "\n",
        "        for filename in os.listdir(input_dir):\n",
        "            if not filename.endswith('.txt'):\n",
        "                continue\n",
        "\n",
        "            total_files += 1\n",
        "\n",
        "            # Create pathnames for input file and output file\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "            output_path = os.path.join(processed_dir, filename)\n",
        "\n",
        "            print(f\"Processing file {filename}\")\n",
        "            stats_file.write(f\"Processing file {filename}\\n\")\n",
        "\n",
        "            file_processed_lines = 0\n",
        "            file_duplicates_removed = 0\n",
        "            file_special_chars_removed = 0\n",
        "            file_malformed_ids_removed = 0\n",
        "            file_malformed_lines_skipped = 0\n",
        "            file_empty_lines_skipped = 0\n",
        "\n",
        "            # Open input file and create output file\n",
        "            try:\n",
        "                with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "\n",
        "                    next(infile)  # Skip the header\n",
        "\n",
        "                    prev_anon_id = None\n",
        "                    prev_query = None\n",
        "\n",
        "                    for line in infile:\n",
        "                        file_processed_lines += 1\n",
        "\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            file_empty_lines_skipped += 1\n",
        "                            continue  # Skip empty lines\n",
        "\n",
        "                        parts = line.split('\\t')\n",
        "                        if len(parts) < 3:\n",
        "                            file_malformed_lines_skipped += 1\n",
        "                            continue  # Skip malformed lines\n",
        "\n",
        "                        anon_id = parts[0].strip()\n",
        "                        query = parts[1].strip()\n",
        "\n",
        "                        if not anon_id.isdigit():\n",
        "                            file_malformed_ids_removed += 1\n",
        "                            malformed_id_file.write(\n",
        "                                f\"{line}\\t{filename}\\n\")\n",
        "                            # Skip malformed anonIDs\n",
        "                            continue\n",
        "\n",
        "                        total_userids.add(anon_id)\n",
        "\n",
        "                        is_duplicate = (\n",
        "                            anon_id == prev_anon_id and query == prev_query)\n",
        "\n",
        "                        is_special_chars_only = bool(\n",
        "                            special_chars_only_pattern.match(query))\n",
        "\n",
        "                        if is_duplicate:\n",
        "                            file_duplicates_removed += 1\n",
        "                        elif is_special_chars_only:\n",
        "                            file_special_chars_removed += 1\n",
        "                            special_char_file.write(line + '\\n')\n",
        "                        else:\n",
        "                            # Only keep a query if its unique and not only consisting of special characters.\n",
        "                            # Modification: Since some rows have 3 columns of data and others 5,\n",
        "                            # we remove the columns for ClickURL and ItemRank so that pandas can create a dataframe\n",
        "                            # from the input data, and since we don't use them anyway.\n",
        "                            outfile.write(anon_id + \"\\t\" + query + '\\n')\n",
        "                            remaining_userids.add(anon_id)\n",
        "\n",
        "                        prev_anon_id = anon_id\n",
        "                        prev_query = query\n",
        "\n",
        "                    total_processed_lines += file_processed_lines\n",
        "                    total_duplicates_removed += file_duplicates_removed\n",
        "                    total_special_chars_removed += file_special_chars_removed\n",
        "                    total_malformed_ids_removed += file_malformed_ids_removed\n",
        "                    total_malformed_lines_skipped += file_malformed_lines_skipped\n",
        "                    total_empty_lines_skipped += file_empty_lines_skipped\n",
        "\n",
        "                    file_stats = [\n",
        "                        f\"  - Processed: {file_processed_lines:,} queries\",\n",
        "                        f\"  - Skipped {file_empty_lines_skipped:,} empty lines\",\n",
        "                        f\"  - Skipped {file_malformed_lines_skipped:,} malformed lines\",\n",
        "                        f\"  - Removed {file_malformed_ids_removed:,} queries with malformed IDs\",\n",
        "                        f\"  - Removed {file_duplicates_removed:,} duplicate queries\",\n",
        "                        f\"  - Removed {file_special_chars_removed:,} special-character-only queries\",\n",
        "                        f\"  - Remaining: {file_processed_lines - file_duplicates_removed - file_special_chars_removed - file_malformed_ids_removed:,} queries\\n\"\n",
        "                    ]\n",
        "\n",
        "                    # Print and write the file stats\n",
        "                    for stat in file_stats:\n",
        "                        print(stat)\n",
        "                        stats_file.write(stat + \"\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"Error processing {filename}: {str(e)}\"\n",
        "                print(error_msg)\n",
        "                stats_file.write(error_msg + \"\\n\")\n",
        "\n",
        "        remaining = total_processed_lines - total_duplicates_removed - \\\n",
        "            total_special_chars_removed - total_malformed_ids_removed\n",
        "\n",
        "        summary_stats = [\n",
        "            \"\\n\" + \"=\"*50,\n",
        "            \"PROCESSING COMPLETE\",\n",
        "            \"=\"*50,\n",
        "            f\"Processed {total_files} files with {total_processed_lines:,} total queries\",\n",
        "            f\"Skipped {total_empty_lines_skipped:,} empty lines\",\n",
        "            f\"Skipped {total_malformed_lines_skipped:,} malformed lines\",\n",
        "            f\"Removed {total_malformed_ids_removed:,} queries with malformed IDs\",\n",
        "            f\"Removed {total_duplicates_removed:,} duplicate queries ({total_duplicates_removed/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Removed {total_special_chars_removed:,} special-char queries ({total_special_chars_removed/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Remaining non-duplicate, valid ID queries: {total_processed_lines - total_duplicates_removed - total_malformed_ids_removed:,} ({(total_processed_lines - total_duplicates_removed - total_malformed_ids_removed)/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Remaining non-duplicate, valid ID, non-special-char queries: {remaining:,} ({remaining/total_processed_lines*100:.2f}%)\",\n",
        "            f\"Removed userIDs: {len(total_userids) - len(remaining_userids)}\",\n",
        "            f\"Remaining UserIDs after processing: {len(remaining_userids)}\",\n",
        "            \"=\"*50,\n",
        "        ]\n",
        "\n",
        "        for stat in summary_stats:\n",
        "            print(stat)\n",
        "            stats_file.write(stat + \"\\n\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"\\nProcessing completed at {datetime.datetime.now()}\\n\")\n",
        "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "        stats_file.write(\n",
        "            f\"\\nProcessing completed at {datetime.datetime.now()}\\n\")\n",
        "        stats_file.write(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Preprocess AOL query log files\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"input_dir\", help=\"Path to input directory containing AOL query log files\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    preprocess_aol_query_log(args.input_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaAvzawvYT87"
      },
      "source": [
        "## Vocabulary Creation ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXnYr-duYT88"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_query(query):\n",
        "    if not isinstance(query, str):\n",
        "        return None\n",
        "    return query.strip().lower()\n",
        "\n",
        "def create_query_vocab(input_dir, vocab_size=None):\n",
        "    query_counts = Counter()\n",
        "    all_queries = []\n",
        "\n",
        "    print(\"🔁 Starting query-level vocabulary creation...\")\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(input_dir, filename)\n",
        "        print(f\"📄 Processing: {filename}\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) < 2:\n",
        "                        continue\n",
        "                    query = clean_query(parts[1])\n",
        "                    if query:\n",
        "                        query_counts.update([query])\n",
        "                        all_queries.append(query)\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing line: {line} - {e}\")\n",
        "                    continue\n",
        "\n",
        "    random.seed(42)\n",
        "    split_index = int(0.8 * len(all_queries))\n",
        "    train_data = all_queries[:split_index]\n",
        "    eval_data = all_queries[split_index:]\n",
        "\n",
        "    with open(TRAIN_FILE_PATH, \"w\", encoding=\"utf-8\") as train_file:\n",
        "        train_file.write(\"\\n\".join(train_data))\n",
        "    with open(EVAL_FILE_PATH, \"w\", encoding=\"utf-8\") as eval_file:\n",
        "        eval_file.write(\"\\n\".join(eval_data))\n",
        "\n",
        "    print(f\"✅ Train set saved to: {TRAIN_FILE_PATH}\")\n",
        "    print(f\"✅ Eval set saved to: {EVAL_FILE_PATH}\")\n",
        "\n",
        "    # === Build Vocabulary ===\n",
        "    most_common_queries = query_counts.most_common(vocab_size) if vocab_size else query_counts.items()\n",
        "    vocab_dict = {query: idx for idx, (query, _) in enumerate(most_common_queries)}\n",
        "    actual_vocab_size = len(vocab_dict)\n",
        "    total_queries = sum(query_counts.values())\n",
        "    covered = sum(query_counts[q] for q in vocab_dict)\n",
        "    coverage = (covered / total_queries) * 100 if total_queries > 0 else 0\n",
        "\n",
        "    vocab_stats = {\n",
        "        \"Requested_Vocabulary_Size\": vocab_size if vocab_size else \"Full\",\n",
        "        \"Actual_Vocabulary_Size\": actual_vocab_size,\n",
        "        \"Total_Queries_Found\": total_queries,\n",
        "        \"Total_Unique_Queries_Found\": len(query_counts),\n",
        "        \"Coverage_Percentage_Of_Top_Queries\": round(coverage, 2),\n",
        "    }\n",
        "\n",
        "    with open(VOCAB_DICT_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(vocab_dict, f)\n",
        "    with open(VOCAB_STATS_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(vocab_stats, f)\n",
        "\n",
        "    print(\"✅ Query-level vocabulary saved to:\", VOCAB_DICT_PATH)\n",
        "    print(\"📊 Vocabulary Stats:\")\n",
        "    print(json.dumps(vocab_stats, indent=4))\n",
        "\n",
        "    return vocab_dict, vocab_stats\n",
        "\n",
        "def get_query_vocabulary(query_file, vocab_size=None):\n",
        "    from collections import Counter\n",
        "    word_counts = Counter()\n",
        "\n",
        "    with open(query_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            query = line.strip().lower()\n",
        "            if query:\n",
        "                word_counts.update([query])  # Treat whole query as a token\n",
        "\n",
        "    most_common = word_counts.most_common(vocab_size) if vocab_size else word_counts.items()\n",
        "    return {query: idx for idx, (query, _) in enumerate(most_common)}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_query_vocab(INPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ElUmifYT88"
      },
      "source": [
        "## Evaluation of n-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHxMUkdNYT88"
      },
      "outputs": [],
      "source": [
        "# @title === N-Gram Model MRR & Accuracy Evaluation ===\n",
        "\n",
        "def query_level_next_prediction(model_path, eval_file, top_k=5, num_examples=10):\n",
        "    model = kenlm.Model(model_path)\n",
        "\n",
        "    # Load query stream\n",
        "    with open(eval_file, 'r', encoding='utf-8') as f:\n",
        "        queries = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"🔍 Running next-query prediction on {num_examples} examples...\\n\")\n",
        "\n",
        "    vocab_dict = get_query_vocabulary(eval_file, vocab_size=None)\n",
        "    vocabulary = list(vocab_dict.keys())\n",
        "\n",
        "    mrr_scores = []\n",
        "\n",
        "    for i in range(len(queries) - 2):\n",
        "        context = f\"{queries[i]} {queries[i+1]}\"  # Trigram context: 2 previous queries\n",
        "        true_query = queries[i+2]\n",
        "\n",
        "        # Score all candidate queries\n",
        "        scores = {\n",
        "            q: model.score(f\"{context} {q}\", bos=False, eos=False)\n",
        "            for q in vocabulary\n",
        "        }\n",
        "\n",
        "        sorted_preds = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_preds = [q for q, _ in sorted_preds[:top_k]]\n",
        "\n",
        "        # Calculate MRR for this prediction\n",
        "        if true_query in top_preds:\n",
        "            rank = top_preds.index(true_query) + 1\n",
        "            mrr = 1.0 / rank\n",
        "        else:\n",
        "            rank = None\n",
        "            mrr = 0.0\n",
        "        mrr_scores.append(mrr)\n",
        "\n",
        "        print(f\"🔹 Example {i+1}\")\n",
        "        print(f\"Context     : [{queries[i]}] → [{queries[i+1]}]\")\n",
        "        print(f\"True query  : {true_query}\")\n",
        "        print(f\"Top-{top_k} : {top_preds}\")\n",
        "        print(f\"MRR         : {mrr:.4f}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if i + 1 == num_examples:\n",
        "            break\n",
        "\n",
        "    avg_mrr = sum(mrr_scores) / len(mrr_scores)\n",
        "    print(f\"\\n📊 Average MRR over {len(mrr_scores)} examples: {avg_mrr:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    query_level_next_prediction(model_path, eval_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_6bTCJxYT89"
      },
      "source": [
        "## Resource Logger and Measuring Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rWE-VK_YT89"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ResourceLogger:\n",
        "    def __init__(self, output_path, interval=5):\n",
        "        self.output_path = os.path.abspath(output_path)\n",
        "        self.output_dir = os.path.dirname(self.output_path)\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        self.interval = interval\n",
        "        self.stop_event = threading.Event()\n",
        "        self.logs = []\n",
        "        self.thread = None\n",
        "        self.start_time = None\n",
        "\n",
        "        with open(self.output_path, 'w') as f:\n",
        "            f.write('[]')\n",
        "\n",
        "    def _collect_resources(self):\n",
        "        elapsed_seconds = time.time() - self.start_time\n",
        "\n",
        "        cpu_percent = psutil.cpu_percent(interval=1)\n",
        "        memory = psutil.virtual_memory()\n",
        "        disk = psutil.disk_usage('/')\n",
        "\n",
        "        gpu_resources = []\n",
        "        try:\n",
        "            command = [\n",
        "                \"nvidia-smi\",\n",
        "                \"--query-gpu=index,name,utilization.gpu,memory.total,memory.used,memory.free\",\n",
        "                \"--format=csv,noheader,nounits\"\n",
        "            ]\n",
        "            result = subprocess.run(\n",
        "                command,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True,\n",
        "                encoding='utf-8'\n",
        "            )\n",
        "            gpu_output = result.stdout.strip()\n",
        "            if gpu_output:\n",
        "                lines = gpu_output.split('\\n')\n",
        "                for line in lines:\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    idx_str, name, util_str, mem_total_str, mem_used_str, mem_free_str = line.split(',')\n",
        "                    gpu_resources.append({\n",
        "                        'gpu_id': int(idx_str.strip()),\n",
        "                        'gpu_name': name.strip(),\n",
        "                        'gpu_load': float(util_str.strip()),\n",
        "                        'gpu_memory_total': int(mem_total_str.strip()),\n",
        "                        'gpu_memory_used': int(mem_used_str.strip()),\n",
        "                        'gpu_memory_free': int(mem_free_str.strip())\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"GPU tracking error: {e}\")\n",
        "\n",
        "        return {\n",
        "            'elapsed_seconds': elapsed_seconds,\n",
        "            'cpu_percent': cpu_percent,\n",
        "            'memory': {\n",
        "                'total': memory.total,\n",
        "                'available': memory.available,\n",
        "                'used': memory.used,\n",
        "                'percent': memory.percent\n",
        "            },\n",
        "            'disk': {\n",
        "                'total': disk.total,\n",
        "                'used': disk.used,\n",
        "                'free': disk.free,\n",
        "                'percent': disk.percent\n",
        "            },\n",
        "            'gpus': gpu_resources\n",
        "        }\n",
        "\n",
        "    def _logging_thread(self):\n",
        "        while not self.stop_event.is_set():\n",
        "            next_log_time = time.time() + self.interval\n",
        "            try:\n",
        "                resource_entry = self._collect_resources()\n",
        "                self.logs.append(resource_entry)\n",
        "                print(f\"Logged resource entry at elapsed time: {resource_entry['elapsed_seconds']:.2f}s\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during resource collection: {e}\")\n",
        "            wait_time = max(0, next_log_time - time.time())\n",
        "            self.stop_event.wait(timeout=wait_time)\n",
        "\n",
        "    def start(self):\n",
        "        if self.thread is not None and self.thread.is_alive():\n",
        "            print(\"Logger already running.\")\n",
        "            return\n",
        "        print(f\"Starting resource logger (interval: {self.interval}s) at {self.output_path}\")\n",
        "        self.stop_event.clear()\n",
        "        self.logs = []\n",
        "        self.start_time = time.time()\n",
        "        self.thread = threading.Thread(target=self._logging_thread, daemon=True)\n",
        "        self.thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.thread is None or not self.thread.is_alive():\n",
        "            print(\"Logger thread not running.\")\n",
        "            return\n",
        "        print(\"Stopping resource logger...\")\n",
        "        self.stop_event.set()\n",
        "        self.thread.join(timeout=5.0)\n",
        "        if self.thread.is_alive():\n",
        "            print(\"Warning: Logger thread did not stop gracefully.\")\n",
        "        self._save_logs()\n",
        "        self.thread = None\n",
        "\n",
        "    def _save_logs(self):\n",
        "        with open(self.output_path, 'w') as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        print(f\"Resource logs saved to {self.output_path} ({len(self.logs)} entries)\")\n",
        "\n",
        "\n",
        "def extract_resource_data(input_file, output_file):\n",
        "    with open(input_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    metrics = []\n",
        "    for entry in data:\n",
        "        metrics.append({\n",
        "            'elapsed_minutes': entry[\"elapsed_seconds\"] / 60.0,\n",
        "            'cpu_percent': entry[\"cpu_percent\"],\n",
        "            'gpu_load': entry[\"gpus\"][0][\"gpu_load\"] if entry[\"gpus\"] else 0,\n",
        "            'disk_percent': entry[\"disk\"][\"percent\"],\n",
        "            'memory_percent': entry[\"memory\"][\"percent\"]\n",
        "        })\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['elapsed_minutes', 'cpu_percent', 'gpu_load', 'disk_percent', 'memory_percent']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for metric in metrics:\n",
        "            writer.writerow(metric)\n",
        "\n",
        "\n",
        "def plot_resource_utilization(data_file, output_name, title):\n",
        "    df = pd.read_csv(data_file)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(df['elapsed_minutes'], df['cpu_percent'], label='CPU Utilization (%)',\n",
        "             color='blue', marker='o', linewidth=2)\n",
        "    plt.plot(df['elapsed_minutes'], df['gpu_load'], label='GPU Utilization (%)',\n",
        "             color='red', marker='s', linewidth=2)\n",
        "    plt.plot(df['elapsed_minutes'], df['disk_percent'], label='Disk Utilization (%)',\n",
        "             color='green', marker='^', linewidth=2)\n",
        "    plt.xlabel('Elapsed Time (minutes)')\n",
        "    plt.ylabel('Utilization Percentage (%)')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.ylim(0, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_name, format='pdf')\n",
        "    print(f\"Plot saved as {output_name}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhyVuZ_8Y1AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OBd7iwuCcIeq",
        "outputId": "d721b75b-4118-4a56-b53a-2cb752de3387"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m553.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp311-cp311-linux_x86_64.whl size=3185035 sha256=4c0f5b6d2c88ff6308b3162fa09c20e52fac9856baf64e2214321c26a4104d88\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-38yqfiig/wheels/4e/ca/6a/e5da175b1396483f6f410cdb4cfe8bc8fa5e12088e91d60413\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y build-essential cmake libboost-all-dev zlib1g-dev libbz2-dev liblzma-dev\n",
        "!git clone https://github.com/kpu/kenlm.git\n",
        "%cd kenlm\n",
        "!mkdir build && cd build && cmake .. && make -j4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Nr-sPcZHcwBd",
        "outputId": "ecc73de2-c596-4230-c2a2-6a99d054ac42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libbz2-dev is already the newest version (1.0.8-5build1).\n",
            "libbz2-dev set to manually installed.\n",
            "liblzma-dev is already the newest version (5.2.5-2ubuntu1).\n",
            "liblzma-dev set to manually installed.\n",
            "libboost-all-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Cloning into 'kenlm'...\n",
            "remote: Enumerating objects: 14185, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 14185 (delta 92), reused 64 (delta 61), pack-reused 14049 (from 4)\u001b[K\n",
            "Receiving objects: 100% (14185/14185), 5.98 MiB | 9.43 MiB/s, done.\n",
            "Resolving deltas: 100% (8048/8048), done.\n",
            "/content/kenlm\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "\u001b[33mCMake Warning (dev) at CMakeLists.txt:101 (find_package):\n",
            "  Policy CMP0167 is not set: The FindBoost module is removed.  Run \"cmake\n",
            "  --help-policy CMP0167\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework\n",
            "-- Found Threads: TRUE\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")\n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\")\n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.5\")\n",
            "-- Looking for clock_gettime in rt\n",
            "-- Looking for clock_gettime in rt - found\n",
            "-- Configuring done (1.2s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-to-string.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/string-to-double.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 53%] Built target kenlm_filter\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 62%] Built target probing_hash_table_benchmark\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 77%] Built target fragment\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 82%] Built target query\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 91%] Built target phrase_table_vocab\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 92%] Built target kenlm_benchmark\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 93%] Built target filter\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 95%] Built target kenlm_builder\n",
            "[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}